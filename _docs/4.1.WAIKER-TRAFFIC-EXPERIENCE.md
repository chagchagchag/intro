---
title: (웨이커) 초당 3000건 이상의 트래픽 처리
category: WAIKER EXPERIENCE
order: 1
---

## 개장 (Market Open) 시점
<img src="https://github.com/chagchagchag/intro/blob/main/_docs/img/4.WAIKER-EXPERIENCE/MARKET-OPEN-AROUND.png?raw=true"/>

장 시작 전 (PRE MARKET) 에는 1초당 1k \~ 2.5k 정도였고 한국시간 기준 22:30 분 이후로 7500건(7.5k) 까지 트래픽이 치솟음을 확인 가능.
<br>


## 폐장 (Market Close) 시점
<img src="https://github.com/chagchagchag/intro/blob/main/_docs/img/4.WAIKER-EXPERIENCE/MARKET-CLOSE-AROUND.png?raw=true"/>

장 마감 시점 전후 에는 1초 당 평균 7500건 (7.5K) 의 트래픽이 치솟음을 확인 가능.


## 동기화/락 없이 시세데이터 각각에 대한 고유키 채번을 통한 동시성 코드로 전환
처음 개발을 시작할 당시 전달받은 Valuesight 라는 레거시 코드에서는 `Lock` 을 사용하는 코드가 있었습니다. 1초에 2000번 이상의 Insert/Update 발생시에 `Lock` 은 점점 백엔드 애플리케이션의 리소스를 갉아먹으면서 결국은 Thread가 꽉 차는 현상이 발생합니다.<br>

첫 입사시에 이런 이유로 인해 개발자분들이 새벽마다 일어나서 서버를 재기동하는 이슈가 있었고, 비어있는 데이터들을 직접 DB에 채워넣는 이슈가 있었습니다. 레거시 서비스인 Valuesight 를 개선할 때에는 락을 배제하고, 레피니티브 시세 데이터 개별 건 각각에 대해 Nanosecond 단위까지는 고유킷값을 가질수 있도록 보장하는 방식으로 채번했고, 또 역으로 분해할 때는 이 고유킷값에서 거래시각/타임존/종목명/국가코드를 분해할 수 있는 구조의 채번 방식을 구현했습니다.<br>

데이터 저장/웹소켓 Push 등의 IO 작업 수행시에는 동기화 작업 없이 고유 킷값을 통해 캐시 자료구조에서 일정 스케쥴링 주기(e.g. 1ms)마다 일정 사이즈 만큼의 시세 데이터들을 인출해서 기준으로 초/분/시/일 집계를 내린 결과를 집계 결과를 저장하는 오프힙 캐시에 저장하는 구조로 IO 작업을 수행하도록 했습니다.<br>

이렇게 했을 때의 장점은 Lock 이 없기에 Thread Full 로 인한 IO 지연 현상은 없었다는 점이었습니다.<br>

다만, 이 당시에는 주어진 시간에 이런 방식으로 결과물을 내었지만, 고도화를 하면서 제품을 발전 시켜나갈 수 있는 여건을 주었다면 아마도 집계된 초/분/시/일 데이터를 캐시에 Queue 자료구조와 Map 자료구조를 통해 FIFO 구조의 집계된 데이터 인출방식을 사용하는 대신에 RabbitMQ 또는 Kafka 에 적재하도록 하고, 네트워크 장애 발생시에만 디스크에 영구 저장이 가능한 오프힙 캐시에 장애 데이터를 적재하도록 구현하지 않았을 까 싶습니다.<br>
<br>

## MVCC 이슈
1초에 최소 2000건 \~ 최대 7000건의 데이터의 insert/update 를 일반적인 관계형 데이터베이스에서 수행한다면 MVCC 상의 단점에 무엇이 단점이 있는지 파악이 필요합니다.<br>

Postgresql 의 경우 MVCC 시에 기존의 Tree 를 Dead Tree로 만들고 새로운 Tree 로 만드는 방식을 채택하고 있기에 Dead Tree 가 계속해서 생겨납니다. 이런 작업이 고반복으로 계속해서 발생한다면 디스크 공간이 부족해 Disk Full 현상이 발생할 수 있는 현상으로 인해 Vaccum을 수행해줘야 하는 이슈가 발생합니다.<br>

당시 Vaccum 에 대해 해결할 수 있는 권한(계정권한)이 있는 인원들은 인프라팀 인원들이었는데, 당시 회사의 사정이 좋지 않아서 대부분 Off 상태인 경우가 많았습니다. 이런 이유로 Vaccum 을 제때에 수행하지 않아 Disk Full 로 인한 데이터 저장 지연 현상이 발생했었습니다.<br>
<br>

## 웹소켓 전송
DB에 데이터를 저장하는 것은 Insert/Update 할 행의 수를 조절해서 Batch 기반의 작업 처리를 하는 것으로 효율적인 처리가 가능하며, 데이터의 조회는 캐시에 업데이트 해두어 API 레벨에서는 캐시의 데이터를 응답하는 방식으로 효율적인 처리가 가능합니다.<br>

하지만, 웹소켓의 경우 세션을 소유한 모든 사용자에게 시세 데이터를 전송해야 했습니다. 그런데 당시 WEB/IOS/AOS 단말 모두에 웹소켓 시세를 전송하는 구조였기에, 스케쥴링 기반의 배치 전송을 한다고 하더라도 사용자가 많아지면 많아질수록 웹소켓 시세데이터를 전송해야 하는 세션들과 전송해야 하는 종목들로 인한 시간 복잡도의 깊이가 계속해서 올라갔습니다. <br>

이런 이유로 언젠가는 웹소켓 인스턴스는 가급적 K8S 기반의 분산환경 또는 Nginx 기반의 다중 인스턴스 소비자로 구성하는 작업을 시작해야 했습니다. <br>

이 당시 개발 기한도 부족하기도 했지만, 개발팀 내의 랩장님, 인프라 팀의 랩장님께 설명을 드려보아도 이 부분에 대해 이해를 도저히 못하셔서 설득을 포기하고 싱글 인스턴스 기반의 웹소켓 인스턴스의 스레드를 최적화 하는 것으로 개발을 마무리 지었었습니다.<br>

이 당시 웹소켓 데이터 전송시 사용한 캐시는 Hazelcast 라는 클러스터링이 가능한 오프힙 캐시를 사용했는데, 이 자료구조를 기반으로 세션과 자료구조들을 공유한다면 시세 데이터의 순서를 유지하면서 클러스터링 기반으로 웹소켓 기능의 클러스터링 구조 전환이 충분히 가능했습니다.<br>

만약, 다시 리팩토링을 하거나 고도화업무를 맡는다면 아마도 이 Websocket 인스턴스에 대해 컨테이너 기반의 클러스터링 구조로 전환을 했을 것 같습니다.<br>
<br>

