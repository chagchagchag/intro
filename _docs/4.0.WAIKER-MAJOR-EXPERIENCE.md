---
title: (웨이커) 주요 문제 해결 경험
category: WAIKER EXPERIENCE
order: 0
---

다른 업무들에 비해 웨이커에서 경험했던 업무 중 데이터 서버 개발업무는 커리어를 통틀어 가장 어려운 업무였습니다. 이런 이유로 간단하게나마 내용을 정리해봐야 겠다는 생각이 들어 정리를 시작하게 되었습니다.<br>
<br>

[경력기술서.pdf](https://github.com/chagchagchag/intro/blob/main/assets/docs/Backend%EA%B0%9C%EB%B0%9C.%EA%B2%BD%EB%A0%A5%EA%B8%B0%EC%88%A0%EC%84%9C.%EC%A0%95%EC%88%9C%EA%B5%AC.pdf)에서 웨이커에서의 기술경험에 대한 내용입니다.<br>
<br>

## 글로벌 서비스 시간데이터 처리
Refinitiv 의 실시간 거래 체결 소켓 데이터가 어느 나라의 주식인지를 판별해 현지 Timezone 을 구하고, 이렇게 구한 Timezone 을 기반으로 해당 데이터가 속한 마켓이 현지 시각 기준으로 장중/마감/장외거래 시각인지 등을 판별하는 기능을 개발했습니다. ZonedDateTime, OffsetDateTime, LocalDateTime 과 Timezone 을 활용한 시간변환 기능들을 상용 서비스를 개발하면서 직접 경험해왔습니다.
<br>
<br>

## 메시지 큐 기반 인스턴스 간 작업 분배 방식 구축
데이터 처리 서버가 하는 역할을 waiker-data-live, waiker-data-collector, waiker-data-websocket 으로 크게 3 가지의 역할로 구분했고, 메시지 큐를 통해서 각각의 인스턴스가 작업을 처리할 수 있도록 인프라를 설계했습니다.<br>
<br>
![](https://github.com/chagchagchag/intro/blob/main/_docs/img/4.WAIKER-EXPERIENCE/PROSUMER-WITH-MQ.png?raw=true)<br>
<br>

MSA 패턴에서 흔히 이야기하는 SAGA, Outbox 를 혼합한 모습이었고, 이렇게 역할을 분리 후에 서로의 인스턴스간의 작업을 메시지큐를 통해 분배하면서, 시스템의 결합도를 낮추고 부하를 분산시킬수 있었습니다.<br>
<br>

## 실시간 거래 체결 데이터 웹소켓 Push, DB 저장 처리
위에서 봤던 전체 구성도에서 각각의 인스턴스 내의 생산자,소비자 구조를 살펴보면 아래와 같습니다. 개별 인스턴스 내에 Offheap 저장소를 이용해 작업 큐를 구성하고 생산자, 소비자 스레드를 별도로 두어 작업을 처리하도록 구성했습니다. 
![](https://github.com/chagchagchag/intro/blob/main/_docs/img/4.WAIKER-EXPERIENCE/OFFHEAP-PRODUCER-CONSUMER.png?raw=true)<br>
<br>

각각의 인스턴스는 생산자 용도의 ExecutorService 로 데이터를 Receive 할 때마다 별도의 처리를 거친 후에 `오프 힙 작업 큐`에 처리한 데이터를 enqueue 합니다. 그리고 소비자는 소비자 스레드 용도의 ExecutorService를 이용해 일정 주기마다 데이터를 일정 사이즈만큼 꺼내어서 작업을 배치처리합니다. 이 당시 Batch 작업의 적당한 스케쥴링 주기를 산출하기 위해 Offheap 캐시에 데이터를 INSERT 하는 속도, DB Insert 속도 등을 측정했고 조금의 여유 스케쥴링 기간을 두어서 스케쥴링 주기를 결정했습니다.<br>
<br>

카프카의 프로듀서, 컨슈머의 내부동작을 보면 스케쥴링 주기, timeout, batchSize 등을 설정하기도 하고, Kafka Streams 의 내부에서는 Offheap 저장소를 사용하기도 하는데, 부하를 줄이기 위해 이런 개념들이 사용되었습니다.<br>
<br>

## 스레드 풀 경량화 작업
Offheap 저장소에 작업큐를 만들어두고 생산자/소비자 구조로 작업을 처리하는 것 외에도 스레드 풀을 용도별로 가벼운 사이즈로 선언해서 사용하는 것 또한 중요했습니다.<br>
<br>

이렇게 스레드 풀을 경량화 하는 것이 가능했던 것은 스케쥴링 기반으로 오프힙 저장소의 작업큐를 생산자/소비자가 각각 enqueue/dequeue 하는 방식으로 애플리케이션의 로직을 구성했기에 가능했습니다. 이렇게 각각의 스레드가 하는 일들이 명확히 구분되어 있어서 스레드 풀을 경량화 하는 데에 도움이 되었습니다.<br>
<br>

waiker-data-collector 를 예로 들어보면 스레드 풀은 아래와 같이 구성했습니다.
- scheduler 스레드 
  - ScheduledExecutorService
  - 전역적으로 대부분의 작업을 스케쥴링하는 데에 공통으로 사용
  - corePoolSize : 2 
- tickInsertExecutor : 일/시/분/초봉 데이터를 DB에 저장하는 역할
  - CompletableFuture가 사용하는 Executor
  - corePoolSize : 1, maxiumPoolSize : 4
- latestInsertExecutor : 현재가격 데이터를 DB Upsert 하는 역할
  - CompletableFuture가 사용하는 Executor
  - corePoolSzie : 1, maximumPoolSize : 2
- pricePushExecutor : 현재가격 데이터를 Websocket MQ에 푸시하는 역할
  - CompletableFuture 가 사용하는 Executor
  - corePoolSize : 2
- listenerExecutor : @RabbitListener 를 이용한 리슨 로직에서의 코드 처리를 비동기적으로 하기 위한 Executor
  - corePoolSize : 1, maximumPoolSize : 2
  - 일/시/분/초봉 가격이 계산된 데이터들을 객체에 저장하고 가격 객체들을 작업 킷값에 맞도록 오프힙 캐시 기반 작업 큐에 저장

<br>
<br>

현업에서는 보통 오래 걸리는 작업에 대해 1차적으로는 스레드 풀을 단순히 크게 잡는 것으로 해결하는 경우가 일반적인데, 미국 주식의 경우는 스레드 풀을 크게 늘리는 것으로 해결이 되지 않아서 위와 같이 스케쥴링 기반의 생산자/소비자 작업큐 구조와 스레드 풀 경량화 작업을 통해 IO 처리의 효율성을 높였습니다.<br>
<br>

k8s와 같은 분산처리 환경을 사용하더라도 가끔은 부하를 처리하는 데에 있어서 Pod 인스턴스 내부에서도 스레드를 경량화해서 처리하는 것 역시 중요하다고 생각합니다. 이런 면에서 웨이커에서의 데이터처리 서버 개발시 스레드 풀 경량화를 했던 경험은 좋은 문제 해결 경험이었다고 생각합니다.
<br>
<br>


## Refinitiv 거래 체결 데이터 로그추출,필드 매핑,검증작업
미국 선물(Future), 장외거래(Pre/Aft) 데이터를 서비스하기로 하면서 기존 데이터와는 필드명/필드값이 전혀 다르게 들어오는 이슈가 있었기에 일봉/시봉/분봉/초봉을 모두 일일이 필드명과 대조해서 어떤 필드명이 거래체결가격인지, 거래 체결 시각은 어떤 필드명으로 매핑되는지 등을 파악해야 했습니다.<br>
<br>

이 과정에서 하루치 장중 데이터를 로깅했고 추출한 데이터를 관련된 주식의 분봉을 보고 대조해가는 과정을 거쳤습니다. 그리고 맞는 필드를 찾은 후에는 로그 파일을 json 기반의 덤프 데이터 파일로 변환했고, 이것을 테스트 코드 기반으로 검증해서 개발을 마무리 지었습니다.<br>
<br>

## 테스트 코드 기반 개발
증권 거래 데이터는 특성상 1ms 에 100건 이상의 데이터가 오기도 하는데, 이런 데이터들을 눈으로 보면서 디버깅하는 것은 불가능합니다. Refinitiv 에서 테스트 서버를 제공하는 것도 아니기에 개발을 하면서 로컬에서 동작을 직접 확인할  수 있는 것도 아닙니다. <br>

이런 이유로 로직의 개발의 시작점은 테스트 코드에서부터 시작하게 되었고 테스트 코드 기반으로 하나의 기능이 완성된 후에는 그 기능을 다시 분리하면서 다시 한번 코드 리팩토링과 테스트 코드 리팩토링을 거쳤습니다. 결과적으로는 이렇게 테스트 코드를 작성해 두어 오류를 검출하기도 쉬워졌고 그 다음 개발 작업 시에 혼동되는 로직을 다시 눈으로 확인하거나 하는 동작없이 필요한 부분의 테스트 코드를 보고 기능을 파악해나갔던 과정을 거치면서 테스트 코드의 장점을 실감했습니다.<br>
<br>

## 회고 
**개발 배경**<br>
이 당시 처음으로 기획조차도 없었던 `레거시 데이터처리 서버`를 직접 기획적인 면까지 직접 파악해서 개발하는 경험을 해봤습니다. 커리어 통틀어서 기획이 없던 업무는 웨이커에서의 프로젝트 경험이 처음이자 마지막일 듯 합니다. 보통 이런 경우에는 결국 모든 책임이 개발자에게 돌아가고, 서비스가 잘 런칭이 되더라도 경영진의 성과로 책정되는 경우가 많은 것 같습니다. <br>

당시 처음으로 넘겨받은 레거시 서비스인 밸류사이트의 시세 데이터 기능은 간단하게 Map 에 `Ticker: 현재가` 형태로 데이터를 담아두는 것으로 인해 시세 데이터가 부정확했고 매번 재기동을 하거나 개발자가 DB의 데이터를 수정하는 식으로 운영해왔던 프로젝트였습니다.<br>

프로젝트를 처음부터 다시 시작하면서 틱 데이터 집계 기능부터 개발을 새로 시작했고 미국주식을 개발하면서는 트래픽에 맞는 인프라 구조로 개편시켜나가는 과정을 거쳤습니다. 열심히 밤을 새워서 몸이 망가질 정도로 일했지만 보상으로 뭐가 얻어지는지 생각을 해봤을 때 웨이커에서는 제가 얻을 수 있는 것도 없었고, 스토킹이나 이런 일들도 많았기에 일단은 퇴사하고 싶다는 생각이 항상 머릿속에 맴돌았던 것 같습니다.<br>

일 자체는 트래픽을 안정적으로 처리할 수 있는 프로젝트라는 점에서 좋아하고 더 발전시키고 싶었는데, 개인적으로 신체적으로 무리가 왔기에 이 당시 업무를 마지막으로 종료한 경험이 있습니다.<br>
<br>

**개선해보고 싶었던 것들**<br>
프로젝트 개편업무를 맡는다면 아마도 위의 시스템에서 `waiker-data-live`, `waiker-data-websocket` 을 분산환경으로 전환해서 애플리케이션의 로드에 따라 클라우드 지출비용을 탄력적으로 축소하고 증가시킬 수 있도록 재구성해서 비용절감이 효율적으로 잘 이뤄지도록 구성했을 듯 싶습니다. 당시에는 랩장님과 인프라팀의 반대에 부딪혀서 도입을 하지 못했는데, 아마도 기존 인력 분들은 아직까지도 도입을 안하고 계실듯 하긴 합니다.<br>
<br>

**리팩토링 부채**<br>
흔히 기술부채라고 하는 이야기를 하는 이야기인데, 제 경우는 상용에 데이터를 릴리즈 한 상태에서 브랜치 관리와 테스트 코드 계획을 통해 상용 릴리즈 계획을 세우면서 점진적으로 제품을 발전시켜나갔습니다. 이 과정에서 초기 개발 시에 투자 일정에 맞추기 위해 개발 속도를 내야만 했기에 Backend 코드 곳곳에 setter, builder 가 퍼져있었습니다. <br>

아마도 지금 시점에서 리팩토링 업무를 맞는다면 도메인과 기획은 이미 파악된 상태이기에 기능별로 계층을 분류할 것 같고 계층 내에 프록시 객체와 팩토리 객체를 용도별로 분류해서 정의한 객체를 Component 로 등록해서 사용할 수 있는 구조로 전환할 것 같고, gradle 멀티 모듈을 통해 더 효율적으로 코드를 관리했을 듯 합니다.<br>
<br>

**배포 프로세스 개선이 필요했다**<br>
초기 개발부터 상용화를 이르게 시작한 후에 발전시켜나갔고, 프로젝트 중반에 들어섰을 때 인프라 팀에서 비전문적인 인력으로 인해 배포 프로세스를 어렵게 만드는 경우가 많았습니다. 이런 이유로 jsch 기반의 gradle 배포를 했었는데, 지금 시점에서 다시 개발한다면 gradle jib 과 ecr, gitlab merge request 등을 통해 백엔드 애플리케이션의 배포가 컨테이너 환경에서 이뤄지도록 했을 것 같습니다.<br>
<br>

긴글 읽어주셔서 감사합니다.<br>
<br>